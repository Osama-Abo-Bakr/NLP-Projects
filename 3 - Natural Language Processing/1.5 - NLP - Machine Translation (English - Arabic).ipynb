{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61db24b4",
   "metadata": {},
   "source": [
    "# Main Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869fff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, nltk\n",
    "\n",
    "# Data Preprocessing\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Building Deep Learning Model\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import tensorflow.keras as k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bc3316",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80027441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>arabic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>مرحبًا.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Run!</td>\n",
       "      <td>اركض!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Duck!</td>\n",
       "      <td>اخفض رأسك!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Duck!</td>\n",
       "      <td>اخفضي رأسك!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Duck!</td>\n",
       "      <td>اخفضوا رؤوسكم!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  english          arabic\n",
       "0     Hi.         مرحبًا.\n",
       "1    Run!           اركض!\n",
       "2   Duck!      اخفض رأسك!\n",
       "3   Duck!     اخفضي رأسك!\n",
       "4   Duck!  اخفضوا رؤوسكم!"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\Data.txt'\n",
    "data = pd.read_csv(path, sep='\\t', names=['english', 'arabic', 'none'])\n",
    "data = data.drop(columns='none', axis=1)\n",
    "data = data.iloc[:4000]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ceaaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english    0\n",
       "arabic     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "268b6745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3a726",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8a5665",
   "metadata": {},
   "source": [
    "## Ecoder For English Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e300b96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAx Length of English Sentence is --->  7\n",
      "The Shape of English padding is --->  (4000, 7)\n",
      "The Number of English Word is -->  1643\n"
     ]
    }
   ],
   "source": [
    "english_text = data.english.tolist()\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(english_text)\n",
    "english_seq = token.texts_to_sequences(english_text)\n",
    "\n",
    "max_eng = max([len(sen) for sen in english_seq])\n",
    "print('The MAx Length of English Sentence is ---> ', max_eng)\n",
    "\n",
    "english_pad = pad_sequences(english_seq, padding='post', maxlen=max_eng)\n",
    "english_pad_array = np.array(english_pad)\n",
    "\n",
    "print('The Shape of English padding is ---> ', english_pad_array.shape)\n",
    "english_dict = token.word_index\n",
    "num_eng_words = len(english_dict) + 1\n",
    "\n",
    "print(\"The Number of English Word is --> \",  num_eng_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff154444",
   "metadata": {},
   "source": [
    "# Decoding Input Arabic Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "060b97f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAx Length of English Sentence is --->  10\n",
      "The Shape of English padding is --->  (4000, 10)\n",
      "The Number of English Word is -->  3996\n"
     ]
    }
   ],
   "source": [
    "arabic_text = []\n",
    "for text in data.arabic:\n",
    "    arabic_text.append('<START>' + text + '<END>')\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(arabic_text)\n",
    "arabic_seq = token.texts_to_sequences(arabic_text)\n",
    "\n",
    "max_ar = max([len(sen) for sen in arabic_seq])\n",
    "print('The MAx Length of English Sentence is ---> ', max_ar)\n",
    "\n",
    "arabic_pad = pad_sequences(arabic_seq, padding='post', maxlen=max_ar)\n",
    "arabic_pad_array = np.array(arabic_pad)\n",
    "\n",
    "print('The Shape of English padding is ---> ', arabic_pad_array.shape)\n",
    "arabic_dict = token.word_index\n",
    "num_ar_words = len(arabic_dict) + 1\n",
    "\n",
    "print(\"The Number of English Word is --> \",  num_ar_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda09f0",
   "metadata": {},
   "source": [
    "# Decoding Output Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb4df266",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output_data = []\n",
    "for text in arabic_seq:\n",
    "    decoder_output_data.append(text[1:])\n",
    "    \n",
    "decoder_padding = pad_sequences(decoder_output_data, padding='post', maxlen=max_ar)\n",
    "decoder_output_data = np.array(to_categorical(decoder_padding, num_ar_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278aa528",
   "metadata": {},
   "source": [
    "# Building Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cad28eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 256)    420608      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 256)    1022976     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 128),        197120      ['embedding[0][0]']              \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 128),  197120      ['embedding_1[0][0]',            \n",
      "                                 (None, 128),                     'lstm[0][1]',                   \n",
      "                                 (None, 128)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, None, 3996)   515484      ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,353,308\n",
      "Trainable params: 2,353,308\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building Encoder Layer\n",
    "\n",
    "encoder_input = Input(shape=(None, ))\n",
    "encoder_embeding = Embedding(num_eng_words, 256, mask_zero=True)(encoder_input)\n",
    "encoder_output, state_h, state_c = LSTM(128, return_state=True)(encoder_embeding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Building Decoder Layer\n",
    "decoder_input = Input(shape=(None, ))\n",
    "decoder_embeding = Embedding(num_ar_words, 256, mask_zero=True)(decoder_input)\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_embeding, initial_state=encoder_states)\n",
    "decoder_dence = Dense(num_ar_words, activation='softmax')\n",
    "output = decoder_dence(decoder_output)\n",
    "\n",
    "# Collect Encoder-Decoder\n",
    "model = Model([encoder_input, decoder_input], output)\n",
    "model.compile(optimizer=k.optimizers.RMSprop(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79ef1389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 11s 53ms/step - loss: 3.4653 - accuracy: 0.2011\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.4882 - accuracy: 0.2372\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.4073 - accuracy: 0.2946\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.3284 - accuracy: 0.3328\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.2539 - accuracy: 0.3721\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 2.1861 - accuracy: 0.4073\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.1246 - accuracy: 0.4226\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.0723 - accuracy: 0.4255\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 2.0294 - accuracy: 0.4278\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.9943 - accuracy: 0.4299\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.9613 - accuracy: 0.4319\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.9295 - accuracy: 0.4358\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.9003 - accuracy: 0.4388\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.8711 - accuracy: 0.4412\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.8441 - accuracy: 0.4440\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.8164 - accuracy: 0.4465\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.7915 - accuracy: 0.4497\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.7677 - accuracy: 0.4521\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.7453 - accuracy: 0.4540\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.7219 - accuracy: 0.4570\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.6989 - accuracy: 0.4600\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.6765 - accuracy: 0.4631\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.6539 - accuracy: 0.4659\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.6283 - accuracy: 0.4700\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.6058 - accuracy: 0.4735\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.5807 - accuracy: 0.4778\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.5554 - accuracy: 0.4837\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 1.5342 - accuracy: 0.4889\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.5105 - accuracy: 0.4929\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.4858 - accuracy: 0.4994\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.4630 - accuracy: 0.5035\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.4399 - accuracy: 0.5067\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.4171 - accuracy: 0.5100\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.3931 - accuracy: 0.5144\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.3693 - accuracy: 0.5210\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.3485 - accuracy: 0.5230\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.3254 - accuracy: 0.5267\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.3019 - accuracy: 0.5316\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.2794 - accuracy: 0.5361\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.2568 - accuracy: 0.5397\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.2360 - accuracy: 0.5449\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.2146 - accuracy: 0.5502\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.1926 - accuracy: 0.5550\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.1708 - accuracy: 0.5589\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.1496 - accuracy: 0.5654\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.1287 - accuracy: 0.5692\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.1091 - accuracy: 0.5744\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.0888 - accuracy: 0.5788\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 1.0693 - accuracy: 0.5852\n",
      "Epoch 50/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.0497 - accuracy: 0.5886\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.0270 - accuracy: 0.5963\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 1.0105 - accuracy: 0.6003\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.9908 - accuracy: 0.6073\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.9719 - accuracy: 0.6115\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.9522 - accuracy: 0.6185\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.9336 - accuracy: 0.6229\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.9177 - accuracy: 0.6282\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8972 - accuracy: 0.6359\n",
      "Epoch 59/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8794 - accuracy: 0.6438\n",
      "Epoch 60/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8621 - accuracy: 0.6512\n",
      "Epoch 61/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8457 - accuracy: 0.6572\n",
      "Epoch 62/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8274 - accuracy: 0.6635\n",
      "Epoch 63/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.8107 - accuracy: 0.6703\n",
      "Epoch 64/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.7935 - accuracy: 0.6792\n",
      "Epoch 65/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.7778 - accuracy: 0.6837\n",
      "Epoch 66/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.7579 - accuracy: 0.6920\n",
      "Epoch 67/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.7451 - accuracy: 0.6979\n",
      "Epoch 68/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.7275 - accuracy: 0.7038\n",
      "Epoch 69/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.7127 - accuracy: 0.7131\n",
      "Epoch 70/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.6971 - accuracy: 0.7196\n",
      "Epoch 71/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.6784 - accuracy: 0.7280\n",
      "Epoch 72/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.6682 - accuracy: 0.7313\n",
      "Epoch 73/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.6491 - accuracy: 0.7417\n",
      "Epoch 74/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.6355 - accuracy: 0.7486\n",
      "Epoch 75/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.6224 - accuracy: 0.7518\n",
      "Epoch 76/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.6051 - accuracy: 0.7616\n",
      "Epoch 77/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5923 - accuracy: 0.7682\n",
      "Epoch 78/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.5794 - accuracy: 0.7750\n",
      "Epoch 79/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5636 - accuracy: 0.7804\n",
      "Epoch 80/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.5532 - accuracy: 0.7857\n",
      "Epoch 81/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.5366 - accuracy: 0.7919\n",
      "Epoch 82/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 34ms/step - loss: 0.5264 - accuracy: 0.7963\n",
      "Epoch 83/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.5103 - accuracy: 0.8043\n",
      "Epoch 84/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.4999 - accuracy: 0.8095\n",
      "Epoch 85/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.4870 - accuracy: 0.8144\n",
      "Epoch 86/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.4749 - accuracy: 0.8194\n",
      "Epoch 87/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.4609 - accuracy: 0.8246\n",
      "Epoch 88/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.4521 - accuracy: 0.8295\n",
      "Epoch 89/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.4400 - accuracy: 0.8342\n",
      "Epoch 90/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.4287 - accuracy: 0.8401\n",
      "Epoch 91/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.4176 - accuracy: 0.8456\n",
      "Epoch 92/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.4077 - accuracy: 0.8492\n",
      "Epoch 93/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.3952 - accuracy: 0.8537\n",
      "Epoch 94/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.3852 - accuracy: 0.8596\n",
      "Epoch 95/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3766 - accuracy: 0.8624\n",
      "Epoch 96/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3645 - accuracy: 0.8681\n",
      "Epoch 97/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3564 - accuracy: 0.8689\n",
      "Epoch 98/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3469 - accuracy: 0.8737\n",
      "Epoch 99/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3354 - accuracy: 0.8788\n",
      "Epoch 100/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3287 - accuracy: 0.8827\n",
      "Epoch 101/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3197 - accuracy: 0.8840\n",
      "Epoch 102/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.3117 - accuracy: 0.8874\n",
      "Epoch 103/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.3009 - accuracy: 0.8936\n",
      "Epoch 104/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.2933 - accuracy: 0.8946\n",
      "Epoch 105/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.2873 - accuracy: 0.8959\n",
      "Epoch 106/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2765 - accuracy: 0.8992\n",
      "Epoch 107/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.2721 - accuracy: 0.9014\n",
      "Epoch 108/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2632 - accuracy: 0.9056\n",
      "Epoch 109/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2555 - accuracy: 0.9055\n",
      "Epoch 110/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2490 - accuracy: 0.9079\n",
      "Epoch 111/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2428 - accuracy: 0.9109\n",
      "Epoch 112/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2356 - accuracy: 0.9130\n",
      "Epoch 113/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2298 - accuracy: 0.9143\n",
      "Epoch 114/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.2225 - accuracy: 0.9155\n",
      "Epoch 115/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2182 - accuracy: 0.9180\n",
      "Epoch 116/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.2110 - accuracy: 0.9186\n",
      "Epoch 117/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.2061 - accuracy: 0.9205\n",
      "Epoch 118/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1997 - accuracy: 0.9217\n",
      "Epoch 119/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1945 - accuracy: 0.9240\n",
      "Epoch 120/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1897 - accuracy: 0.9230\n",
      "Epoch 121/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1844 - accuracy: 0.9267\n",
      "Epoch 122/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1800 - accuracy: 0.9263\n",
      "Epoch 123/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1744 - accuracy: 0.9278\n",
      "Epoch 124/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1703 - accuracy: 0.9308\n",
      "Epoch 125/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1665 - accuracy: 0.9297\n",
      "Epoch 126/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1612 - accuracy: 0.9319\n",
      "Epoch 127/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1576 - accuracy: 0.9325\n",
      "Epoch 128/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1528 - accuracy: 0.9341\n",
      "Epoch 129/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1502 - accuracy: 0.9360\n",
      "Epoch 130/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.1453 - accuracy: 0.9360\n",
      "Epoch 131/200\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 0.1416 - accuracy: 0.9373\n",
      "Epoch 132/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1388 - accuracy: 0.9366\n",
      "Epoch 133/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1343 - accuracy: 0.9396\n",
      "Epoch 134/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1324 - accuracy: 0.9380\n",
      "Epoch 135/200\n",
      "14/14 [==============================] - 0s 36ms/step - loss: 0.1283 - accuracy: 0.9406\n",
      "Epoch 136/200\n",
      "14/14 [==============================] - 1s 36ms/step - loss: 0.1248 - accuracy: 0.9401\n",
      "Epoch 137/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1228 - accuracy: 0.9412\n",
      "Epoch 138/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.1193 - accuracy: 0.9436\n",
      "Epoch 139/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.1163 - accuracy: 0.9429\n",
      "Epoch 140/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1138 - accuracy: 0.9428\n",
      "Epoch 141/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1110 - accuracy: 0.9452\n",
      "Epoch 142/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.1092 - accuracy: 0.9431\n",
      "Epoch 143/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1068 - accuracy: 0.9451\n",
      "Epoch 144/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1034 - accuracy: 0.9456\n",
      "Epoch 145/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.1016 - accuracy: 0.9457\n",
      "Epoch 146/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0995 - accuracy: 0.9459\n",
      "Epoch 147/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0969 - accuracy: 0.9469\n",
      "Epoch 148/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0955 - accuracy: 0.9454\n",
      "Epoch 149/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0928 - accuracy: 0.9472\n",
      "Epoch 150/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0914 - accuracy: 0.9478\n",
      "Epoch 151/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0891 - accuracy: 0.9476\n",
      "Epoch 152/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0876 - accuracy: 0.9477\n",
      "Epoch 153/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0854 - accuracy: 0.9491\n",
      "Epoch 154/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0838 - accuracy: 0.9492\n",
      "Epoch 155/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0825 - accuracy: 0.9486\n",
      "Epoch 156/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0810 - accuracy: 0.9490\n",
      "Epoch 157/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0791 - accuracy: 0.9493\n",
      "Epoch 158/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0778 - accuracy: 0.9498\n",
      "Epoch 159/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0762 - accuracy: 0.9495\n",
      "Epoch 160/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0751 - accuracy: 0.9512\n",
      "Epoch 161/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0737 - accuracy: 0.9499\n",
      "Epoch 162/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0725 - accuracy: 0.9500\n",
      "Epoch 163/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0712 - accuracy: 0.9509\n",
      "Epoch 164/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0698 - accuracy: 0.9507\n",
      "Epoch 165/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0689 - accuracy: 0.9505\n",
      "Epoch 166/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0677 - accuracy: 0.9509\n",
      "Epoch 167/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0665 - accuracy: 0.9504\n",
      "Epoch 168/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0658 - accuracy: 0.9505\n",
      "Epoch 169/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0645 - accuracy: 0.9522\n",
      "Epoch 170/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0636 - accuracy: 0.9505\n",
      "Epoch 171/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0627 - accuracy: 0.9512\n",
      "Epoch 172/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0618 - accuracy: 0.9522\n",
      "Epoch 173/200\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0609 - accuracy: 0.9504\n",
      "Epoch 174/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0601 - accuracy: 0.9510\n",
      "Epoch 175/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0593 - accuracy: 0.9504\n",
      "Epoch 176/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0584 - accuracy: 0.9507\n",
      "Epoch 177/200\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 0.0576 - accuracy: 0.9506\n",
      "Epoch 178/200\n",
      "14/14 [==============================] - 1s 37ms/step - loss: 0.0571 - accuracy: 0.9514\n",
      "Epoch 179/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0563 - accuracy: 0.9502\n",
      "Epoch 180/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0556 - accuracy: 0.9508\n",
      "Epoch 181/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0548 - accuracy: 0.9517\n",
      "Epoch 182/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0542 - accuracy: 0.9509\n",
      "Epoch 183/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0536 - accuracy: 0.9508\n",
      "Epoch 184/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0530 - accuracy: 0.9511\n",
      "Epoch 185/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0526 - accuracy: 0.9510\n",
      "Epoch 186/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0518 - accuracy: 0.9514\n",
      "Epoch 187/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0512 - accuracy: 0.9512\n",
      "Epoch 188/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0509 - accuracy: 0.9521\n",
      "Epoch 189/200\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0504 - accuracy: 0.9502\n",
      "Epoch 190/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0498 - accuracy: 0.9516\n",
      "Epoch 191/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0492 - accuracy: 0.9512\n",
      "Epoch 192/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0490 - accuracy: 0.9510\n",
      "Epoch 193/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0484 - accuracy: 0.9508\n",
      "Epoch 194/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0479 - accuracy: 0.9519\n",
      "Epoch 195/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0475 - accuracy: 0.9503\n",
      "Epoch 196/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0472 - accuracy: 0.9519\n",
      "Epoch 197/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0468 - accuracy: 0.9512\n",
      "Epoch 198/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0462 - accuracy: 0.9518\n",
      "Epoch 199/200\n",
      "14/14 [==============================] - 0s 35ms/step - loss: 0.0460 - accuracy: 0.9512\n",
      "Epoch 200/200\n",
      "14/14 [==============================] - 0s 34ms/step - loss: 0.0456 - accuracy: 0.9516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2369380b9c8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([english_pad_array,arabic_pad_array], decoder_output_data, epochs=200, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f64c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_refrece_model():\n",
    "    encoder_model_refrence = Model(encoder_input, encoder_states)\n",
    "    \n",
    "    decoder_state_h = Input(shape=(128, ))\n",
    "    decoder_state_c = Input(shape=(128, ))\n",
    "    decoder_inputs_states = [decoder_state_h, decoder_state_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(decoder_embeding, initial_state=decoder_inputs_states)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    \n",
    "    decoder_outputs = decoder_dence(decoder_outputs)\n",
    "    decoder_model_refrence = Model([decoder_input] + decoder_inputs_states,\n",
    "                                   [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model_refrence, decoder_model_refrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93b17117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_token(sen):\n",
    "    words = sen.lower().split()\n",
    "    token_list = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            token_list.append(english_dict[word])\n",
    "        except:\n",
    "            print(\"THe sentence is not recognize \")\n",
    "            run()\n",
    "    return pad_sequences([token_list], maxlen=max_eng, padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ff53d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\encoder_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\encoder_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\decoder_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\decoder_model\\assets\n"
     ]
    }
   ],
   "source": [
    "enc_model, dec_model = make_refrece_model()\n",
    "enc_model.save(r\"D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\encoder_model\")\n",
    "enc_model.save(r\"D:\\Courses language programming\\LLM - Transformer - NLP\\NLP - Complete Course\\Data\\ara-eng\\decoder_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21a59585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    enc_model, dec_model = make_refrece_model()\n",
    "\n",
    "    for sen in range(english_pad_array.shape[0]):\n",
    "        states_values = enc_model.predict(str_to_token(input(\"Enter an English Sentence:  \")))\n",
    "\n",
    "        empty_target = np.zeros((1, 1))\n",
    "        empty_target[0, 0] = arabic_dict[\"start\"]\n",
    "        stopping_condition = False\n",
    "        decoded_translation = \"\"\n",
    "        while not stopping_condition:\n",
    "            dec_output, h, c = dec_model.predict([empty_target] + states_values)\n",
    "            sampled_word_index = np.argmax(dec_output[0, -1, :])\n",
    "            sampled_word = None\n",
    "            for word, index in arabic_dict.items():\n",
    "                if sampled_word_index == index:\n",
    "                    decoded_translation += \" \" + word\n",
    "                    sampled_word = word\n",
    "                if sampled_word == \"end\" or len(decoded_translation.split()) > max_ar:\n",
    "                    stopping_condition = True\n",
    "\n",
    "            empty_target = np.zeros((1, 1))\n",
    "            empty_target[0, 0] = sampled_word_index\n",
    "            states_values = [h, c]\n",
    "\n",
    "        print(decoded_translation[:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86baad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
